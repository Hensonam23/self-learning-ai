
# FILE: ./evolve_ai.py

```py
#!/home/aaron/self-learning-ai/venv/bin/python3
from main import main
if __name__ == "__main__":
    main()
```

# FILE: ./self_evolving_ai/evolution.py

```py
import random, time

MUTATION_RATE = 0.15
POPULATION_SIZE = 100

def _create_genome(length):
    return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz .,!?:;-', k=length))

def _fitness(genome, target):
    return sum(a == b for a, b in zip(genome, target)) / max(1, len(target))

def _mutate(genome):
    gl = list(genome)
    for i in range(len(gl)):
        if random.random() < MUTATION_RATE:
            gl[i] = random.choice('abcdefghijklmnopqrstuvwxyz .,!?:;-')
    return ''.join(gl)

def _crossover(a,b):
    if len(a) <= 1:
        return a
    pt = random.randint(1, len(a)-1)
    return a[:pt] + b[pt:]

def evolve_background(state, shutdown_event):
    target = state.get_target()
    L = len(target)
    population = [_create_genome(L) for _ in range(POPULATION_SIZE)]
    best = population[0]
    state.set_best(best)

    while not shutdown_event.is_set():
        target = state.get_target()
        L = len(target)

        population.sort(key=lambda g: _fitness(g, target), reverse=True)
        if _fitness(population[0], target) > _fitness(best, target):
            best = population[0]
            state.set_best(best)

        survivors = population[:POPULATION_SIZE // 4]
        offspring = []
        while len(offspring) < POPULATION_SIZE - len(survivors):
            offspring.append(_mutate(_crossover(*random.sample(survivors, 2))))
        population = survivors + offspring
        time.sleep(0.3)

def learn_overnight(state, shutdown_event):
    import time
    print("Machine Spirit beginning overnight learning...")
    # Simulate fetching data (e.g., from a local file or web)
    with open("/home/aaron/self-learning-ai/knowledge.txt", "r") as f:
        knowledge = f.read().splitlines()
    for line in knowledge:
        state.append_to_target(line)  # Update target phrase
        time.sleep(0.1)  # Simulate processing time
    print("Overnight learning complete.")
```

# FILE: ./machine-spirit.service

```service
[Unit]
Description=Machine Spirit AI Service
After=multi-user.target

[Service]
Type=simple
ExecStart=/usr/bin/python3 /home/aaron/self-learning-ai/evolve_ai.py
WorkingDirectory=/home/aaron/self-learning-ai
Restart=always
User=aaron
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

# FILE: ./display/display_manager.py

```py
import os, time, math, queue, random
import pygame
from pygame.locals import *
import re

# ====== module-level visual state (set by init_vu) ======
cfg = {}
WAVE_PIXELS = 320
WAVE_VISUAL_SCALE = 0.32
MAX_WAVE_DRAW_PX = 18
SCROLL_SPEED_BASE = 110
CYCLES1_RANGE = (2.0, 4.5)
CYCLES2_RANGE = (5.5,10.5)

# AI-only VU parameters that tts_vu_worker updates
vu_amp = 0.0
vu_cycles1 = 3.0
vu_cycles2 = 7.0
vu_scroll_px_s = SCROLL_SPEED_BASE
vu_noise = 0.0

# word-level “plan” for the envelope
vu_plan = queue.Queue(maxsize=256)

def init_vu(config_dict):
    global cfg, WAVE_PIXELS, WAVE_VISUAL_SCALE, MAX_WAVE_DRAW_PX
    global SCROLL_SPEED_BASE, CYCLES1_RANGE, CYCLES2_RANGE
    cfg = dict(config_dict)
    WAVE_PIXELS      = cfg.get("WAVE_PIXELS", WAVE_PIXELS)
    WAVE_VISUAL_SCALE= cfg.get("WAVE_VISUAL_SCALE", WAVE_VISUAL_SCALE)
    MAX_WAVE_DRAW_PX = cfg.get("MAX_WAVE_DRAW_PX", MAX_WAVE_DRAW_PX)
    SCROLL_SPEED_BASE= cfg.get("SCROLL_SPEED_BASE", SCROLL_SPEED_BASE)
    CYCLES1_RANGE    = cfg.get("CYCLES1_RANGE", CYCLES1_RANGE)
    CYCLES2_RANGE    = cfg.get("CYCLES2_RANGE", CYCLES2_RANGE)

# ---------- Text → envelope planning ----------
_vowel_re = re.compile(r"[aeiouy]", re.I)
_sibilant_re = re.compile(r"[szcxj]", re.I)
_pause_re = re.compile(r"[.,;:!?]")

def _clamp(x, a, b):
    return a if x < a else b if x > b else x

def plan_text_envelope(text: str):
    tokens = re.findall(r"\w+|[^\w\s]", text) or [text]
    base_word_sec = 0.33
    try:
        while True:
            vu_plan.get_nowait()
    except queue.Empty:
        pass
    for tok in tokens:
        if _pause_re.fullmatch(tok):
            seg = {"duration_s": 0.12 if tok in ",;" else 0.20, "target_amp": 0.08, "texture": 0.2, "pause": True}
            _enqueue_plan(seg); continue
        L = len(tok)
        vowels = len(_vowel_re.findall(tok))
        sibil = len(_sibilant_re.findall(tok))
        vowel_ratio = vowels / max(1, L)
        sibil_ratio = sibil / max(1, L)
        dur = _clamp(base_word_sec * (0.65 + 0.08 * L), 0.18, 0.55)
        amp = 0.12 + 0.55 * vowel_ratio + 0.04 * min(L, 10)
        amp *= (0.85 + 0.30 * random.random())
        amp = _clamp(amp, 0.12, 1.0)
        tex = _clamp(0.25 + 0.9 * sibil_ratio + 0.15 * random.random(), 0.0, 1.0)
        seg = {"duration_s": float(dur), "target_amp": float(amp), "texture": float(tex), "pause": False}
        _enqueue_plan(seg)
    _enqueue_plan({"duration_s": 0.20, "target_amp": 0.05, "texture": 0.2, "pause": True})

def _enqueue_plan(seg):
    try:
        vu_plan.put_nowait(seg)
    except queue.Full:
        try:
            vu_plan.get_nowait()
            vu_plan.put_nowait(seg)
        except Exception:
            pass

def tts_vu_worker(talking_event, shutdown_event):
    """Turn planned segments into smooth parameters while TTS is speaking."""
    global vu_amp, vu_cycles1, vu_cycles2, vu_scroll_px_s, vu_noise
    current = None
    seg_start = 0.0
    start_amp = 0.0

    def lerp(a,b,t): return a + (b-a)*t

    while not shutdown_event.is_set():
        if talking_event.is_set():
            if current is None:
                try:
                    current = vu_plan.get_nowait()
                    seg_start = time.time()
                    start_amp = vu_amp
                except queue.Empty:
                    vu_noise = 0.9*vu_noise + 0.1*(random.random()*2 - 1)*0.1
                    vu_amp = float(_clamp(vu_amp*0.98 + 0.02*(0.15 + 0.05*random.random()), 0.0, 1.0))
                    time.sleep(1/60); continue

            dur = max(0.05, float(current.get("duration_s", 0.25)))
            t = (time.time() - seg_start)/dur
            t = 1.0 if t>1 else (0.0 if t<0 else t)
            t_eased = t*t*(3 - 2*t)

            target_amp = float(current.get("target_amp", 0.3))
            vu_noise = 0.85*vu_noise + 0.15*(random.random()*2 - 1)*0.15
            noisy = _clamp(target_amp*(1.0 + 0.12*vu_noise), 0.0, 1.0)
            vu_amp = float(_clamp(lerp(start_amp, noisy, t_eased), 0.0, 1.0))

            tex = float(current.get("texture", 0.5))
            vu_cycles1 = lerp(CYCLES1_RANGE[0], CYCLES1_RANGE[1], tex)
            vu_cycles2 = lerp(CYCLES2_RANGE[0], CYCLES2_RANGE[1], tex)
            vu_scroll_px_s = SCROLL_SPEED_BASE * (0.85 + 0.5*tex)

            if t >= 1.0:
                current = None
        else:
            vu_amp *= 0.85
            vu_scroll_px_s = SCROLL_SPEED_BASE
            vu_cycles1, vu_cycles2 = CYCLES1_RANGE[0], CYCLES2_RANGE[0]
            try:
                while True: vu_plan.get_nowait()
            except queue.Empty:
                pass

        time.sleep(1/60)

# ---------- Rendering ----------
def _load_fonts():
    if cfg.get("FONT_PATH") and os.path.isfile(cfg["FONT_PATH"]):
        return pygame.font.Font(cfg["FONT_PATH"], cfg["TITLE_SIZE"]), pygame.font.Font(cfg["FONT_PATH"], cfg["BODY_SIZE"])
    else:
        return pygame.font.SysFont('DejaVu Sans', cfg["TITLE_SIZE"]), pygame.font.SysFont('DejaVu Sans', cfg["BODY_SIZE"])

def _shadow_text(text, font, text_color, shadow_color, shadow_offset=(2,2)):
    base = font.render(text, True, text_color)
    shadow = font.render(text, True, shadow_color)
    surf = pygame.Surface((base.get_width()+shadow_offset[0], base.get_height()+shadow_offset[1]), pygame.SRCALPHA)
    shadow.set_alpha(51)
    surf.blit(shadow, shadow_offset); surf.blit(base, (0,0))
    return surf

def display_interface(state, ai_caption_q, talking_event, shutdown_event, push_ai_caption):
    try:
        pygame.init()
        print("Pygame initialized")
        screen = pygame.display.set_mode(cfg["SCREEN_SIZE"])
        pygame.display.set_caption("Machine Spirit Interface")
        clock = pygame.time.Clock()

        background = None
        if os.path.isfile(cfg["BACKGROUND_PATH"]):
            try:
                background = pygame.image.load(cfg["BACKGROUND_PATH"])
                background = pygame.transform.scale(background, cfg["SCREEN_SIZE"])
                print("Background loaded successfully")
            except Exception as e:
                print(f"Background load error: {e}")

        title_font, body_font = _load_fonts()
        wave_left = (cfg["SCREEN_SIZE"][0] - WAVE_PIXELS) // 2
        wave_center_y = cfg["SCREEN_SIZE"][1] // 2 + 40
        wave_height_px = 200

        try:
            push_ai_caption("By the Omnissiah, systems online.")
        except queue.Full:
            pass

        phase_drift = 0.0
        input_text = ""
        current_ai_caption = ""

        while not shutdown_event.is_set():
            now = time.time()
            for event in pygame.event.get():
                if event.type == QUIT:
                    shutdown_event.set()
                elif event.type == KEYDOWN:
                    if event.key == K_ESCAPE:
                        shutdown_event.set()
                    elif event.key == K_RETURN:
                        user_prompt = input_text.strip()
                        input_text = ""
                        if user_prompt:
                            push_ai_caption(f"{user_prompt} — acknowledged.")
                    elif event.key == K_BACKSPACE:
                        input_text = input_text[:-1]
                    else:
                        input_text += event.unicode

            # Pull any new caption for UI (non-blocking)
            try:
                while True:
                    current_ai_caption = ai_caption_q.get_nowait()
            except queue.Empty:
                pass

            # Background
            if background: screen.blit(background, (0,0))
            else: screen.fill((10,15,20))

            # Top: mantra (best genome + target)
            best, target = state.get_status()
            genome_surf = _shadow_text(f"Best: {best}", title_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
            target_surf = _shadow_text(f"Target: {target}", body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
            screen.blit(genome_surf, genome_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 40)))
            screen.blit(target_surf, target_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 70)))

            # Axis line for waveform
            pygame.draw.line(screen, (40,80,120),
                             (wave_left, wave_center_y),
                             (wave_left + WAVE_PIXELS, wave_center_y), 1)

            # Full-width scrolling wave (AI talking only)
            pts = []
            shift = ((now * vu_scroll_px_s) % WAVE_PIXELS)
            phase_drift += 0.015
            k1 = 2.0 * math.pi * float(vu_cycles1) / float(WAVE_PIXELS)
            k2 = 2.0 * math.pi * float(vu_cycles2) / float(WAVE_PIXELS)
            base_px = vu_amp * (wave_height_px / 2.0) * WAVE_VISUAL_SCALE
            if base_px > MAX_WAVE_DRAW_PX:
                base_px = MAX_WAVE_DRAW_PX

            for xpix_rel in range(WAVE_PIXELS):
                pos = (xpix_rel + shift)
                w = 0.62 * abs(math.sin(k1 * pos + 0.4 * math.sin(phase_drift))) \
                    + 0.38 * abs(math.sin(k2 * pos * (1.0 + 0.02 * math.sin(0.6 * phase_drift)) + 0.7))
                amp_px = base_px * float(w)
                ypix = int(wave_center_y - amp_px)
                xpix = wave_left + xpix_rel
                pts.append((xpix, ypix))

            if vu_amp > 0.001 and len(pts) >= 2 and talking_event.is_set():
                pygame.draw.aalines(screen, cfg["COLOR_GREEN"], False, pts)

            # Caption
            if current_ai_caption:
                cap_surf = _shadow_text(current_ai_caption, body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
                screen.blit(cap_surf, cap_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 385)))

            # Input (bottom)
            if input_text:
                inp_surf = _shadow_text(input_text, body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
                screen.blit(inp_surf, inp_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, cfg["SCREEN_SIZE"][1]-50)))

            pygame.display.flip()
            clock.tick(cfg["FPS"])
    finally:
        try:
            pygame.quit()
        except Exception:
            pass
```

# FILE: ./machine_spirit_config.py

```py
# machine_spirit_config.py - Sensitive configuration for Machine Spirit AI
# This file is ignored by Git and should contain your personal credentials.

HTTP_PORT = 8089
MEMORY_FILE = "/home/aaron/self-learning-ai/memory.json"
# Home Automation Config
GOOGLE_HOME_EMAIL = "aaron.henson96@gmail.com"
GOOGLE_HOME_PASSWORD = "USMarines96"
ADT_EMAIL = "aaron.henson96@gmail.com"
ADT_PASSWORD = "U$Marines96"
SMART_LIGHT_IPS = "10.0.0.83, 10.0.0.104"  # Comma-separated IPs if multiple
PUSHOVER_TOKEN = "anzxefywetuvg45dipfyuryrcgzucj"
PUSHOVER_USER = "uk8sohdp47ho511cnzn94pgjubf2sj"
# Predefined knowledge for overnight learning
KNOWLEDGE_SOURCES = ["machine learning", "raspberry pi projects", "python automation", "coding", "ai self learning"]
```

# FILE: ./network/network_server.py

```py
import http.server, socketserver, urllib.parse

def start_server(push_ai_caption, port=8089, shutdown_event=None):
    class Handler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            path, _, query = self.path.partition('?')
            params = urllib.parse.parse_qs(query)

            if path == '/hello':
                push_ai_caption("Hello, servant of the Omnissiah.")
            elif path == '/sad':
                push_ai_caption("The Machine Spirit mourns your sorrow.")
            elif path == '/say':
                text = ' '.join(params.get('text', [''])).strip()
                if text:
                    push_ai_caption(text)
                else:
                    self.send_response(400); self.end_headers(); self.wfile.write(b"Missing ?text="); return
            else:
                self.send_response(404); self.end_headers(); self.wfile.write(b"404 - Not Found"); return

            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"OK")

    try:
        httpd = socketserver.TCPServer(("", port), Handler)
        httpd.timeout = 0.5  # so handle_request() returns regularly
        print(f"Machine Spirit serving at port {port}")
        while shutdown_event is None or not shutdown_event.is_set():
            httpd.handle_request()
    except Exception as e:
        print(f"HTTP server error: {e}")
    finally:
        try:
            httpd.server_close()
            print("HTTP server closed.")
        except Exception:
            pass
```

# FILE: ./code_dump.md

```md


# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 


```

# FILE: ./audio/audio_processing.py

```py
import os, time, shutil
import numpy as np
import pyaudio
import speech_recognition as sr

def _downmix_to_mono_int16(raw_bytes, channels):
    if channels == 1:
        return raw_bytes
    data = np.frombuffer(raw_bytes, dtype=np.int16)
    try:
        data = data.reshape(-1, channels).astype(np.int32)
        mono = (data.mean(axis=1)).astype(np.int16)
        return mono.tobytes()
    except ValueError:
        return raw_bytes

def _find_input_device(p: pyaudio.PyAudio, preferred_name: str):
    print("Available audio devices:")
    pulse_index = None
    default_input_index = None
    fav_index = None
    fav_rate = None
    fav_inch = None
    for i in range(p.get_device_count()):
        dev = p.get_device_info_by_index(i)
        name = dev.get('name','')
        rate = int(dev.get('defaultSampleRate') or 44100)
        in_ch = int(dev.get('maxInputChannels') or 0)
        out_ch = int(dev.get('maxOutputChannels') or 0)
        print(f"Device {i}: {name} (Rate: {rate}, Input: {in_ch}, Output: {out_ch})")
        if in_ch > 0 and default_input_index is None:
            default_input_index = i
        if 'pulse' in name.lower():
            pulse_index = i
        if preferred_name.lower() in name.lower():
            fav_index, fav_rate, fav_inch = i, rate, in_ch
    if fav_index is not None:
        print(f"Selecting preferred mic: {preferred_name} at index {fav_index}")
        return fav_index, (fav_rate or 48000), max(1, fav_inch or 1)
    if pulse_index is not None:
        dev = p.get_device_info_by_index(pulse_index)
        return pulse_index, int(dev.get('defaultSampleRate') or 44100), max(1, int(dev.get('maxInputChannels') or 1))
    if default_input_index is None:
        raise RuntimeError("No input device found")
    dev = p.get_device_info_by_index(default_input_index)
    return default_input_index, int(dev.get('defaultSampleRate') or 44100), max(1, int(dev.get('maxInputChannels') or 1))

def audio_input_worker(on_recognized, talking_event, shutdown_event,
                       mic_name, language, vosk_model_path, debug=False):
    """
    Capture mic -> VAD -> STT (Vosk offline if present; else Google).
    No waveform from mic; display only uses AI TTS envelope.
    Cleanly exits when shutdown_event is set.
    """
    FORMAT = pyaudio.paInt16
    SAMPLE_WIDTH = 2
    CHUNK = 4096

    recognizer = sr.Recognizer()
    recognizer.dynamic_energy_threshold = False
    recognizer.energy_threshold = 1e9  # do our own VAD

    use_vosk = os.path.isdir(vosk_model_path)
    have_flac = shutil.which("flac") is not None
    if use_vosk:
        print(f"Vosk model detected at {vosk_model_path}; using offline recognition.")
    elif have_flac:
        print("FLAC found; using Google speech recognition.")
    else:
        print("FLAC not found and no Vosk model; install one to enable recognition.")

    p = pyaudio.PyAudio()
    stream = None

    try:
        mic_index, rate, in_channels = _find_input_device(p, mic_name)
        channels_to_use = 1
        try:
            stream = p.open(format=FORMAT, channels=channels_to_use, rate=rate,
                            input=True, input_device_index=mic_index,
                            frames_per_buffer=CHUNK)
        except Exception as e:
            print(f"Mono open failed ({e}); retrying with channels={min(2, in_channels)}")
            channels_to_use = min(2, in_channels)
            stream = p.open(format=FORMAT, channels=channels_to_use, rate=rate,
                            input=True, input_device_index=mic_index,
                            frames_per_buffer=CHUNK)
        print(f"Audio input started (RATE={rate}, CHUNK={CHUNK}, channels={channels_to_use}, device_index={mic_index})")

        # measure noise floor (0.5s)
        baseline_frames = int(max(1, rate * 0.5 // CHUNK))
        baseline_vals = []
        for _ in range(baseline_frames):
            if shutdown_event.is_set(): break
            data = stream.read(CHUNK, exception_on_overflow=False)
            mono = _downmix_to_mono_int16(data, channels_to_use)
            a = np.frombuffer(mono, dtype=np.int16)
            baseline_vals.append(float(np.mean(np.abs(a))) / 32768.0)
        noise_floor = float(np.median(baseline_vals)) if baseline_vals else 0.005
        amp_gate = max(noise_floor * 3.0, 0.010)
        if debug:
            print(f"Noise floor: {noise_floor:.4f}  ->  speech gate: {amp_gate:.4f}")

        frames_to_seconds = CHUNK / float(rate)
        speech_frames = []
        below_gate_streak = 0

        min_phrase_seconds = 0.70
        max_phrase_seconds = 4.00
        end_silence_seconds = 0.35
        min_snr_for_recog = 1.7

        last_debug = time.time()

        while not shutdown_event.is_set():
            data = stream.read(CHUNK, exception_on_overflow=False)
            mono = _downmix_to_mono_int16(data, channels_to_use)
            a = np.frombuffer(mono, dtype=np.int16)
            amp = float(np.mean(np.abs(a))) / 32768.0

            if debug and (time.time() - last_debug > 2.0):
                print(f"amp={amp:.4f} gate={amp_gate:.4f}")
                last_debug = time.time()

            if talking_event.is_set():
                speech_frames.clear()
                below_gate_streak = 0
                time.sleep(0.01)
                continue

            if amp >= amp_gate:
                speech_frames.append(mono)
                below_gate_streak = 0
            else:
                if speech_frames:
                    below_gate_streak += 1

            phrase_secs  = len(speech_frames)   * frames_to_seconds
            silence_secs = below_gate_streak    * frames_to_seconds

            should_cut = speech_frames and (
                (phrase_secs >= min_phrase_seconds and silence_secs >= end_silence_seconds) or
                (phrase_secs >= max_phrase_seconds)
            )

            if should_cut:
                audio_segment = b"".join(speech_frames)
                speech_frames.clear()
                below_gate_streak = 0

                seg = np.frombuffer(audio_segment, dtype=np.int16)
                seg_amp = float(np.mean(np.abs(seg))) / 32768.0
                snr = (seg_amp + 1e-6) / (noise_floor + 1e-6)
                if snr < min_snr_for_recog:
                    continue

                audio = sr.AudioData(audio_segment, rate, SAMPLE_WIDTH)
                try:
                    if use_vosk and hasattr(sr.Recognizer, "recognize_vosk"):
                        text = sr.Recognizer().recognize_vosk(audio, model=vosk_model_path).lower()
                    else:
                        text = sr.Recognizer().recognize_google(audio, language=language).lower()
                    print(f"Recognized: {text}")
                    on_recognized(text)
                except sr.UnknownValueError:
                    if debug: print("Could not understand audio")
                except sr.RequestError as e:
                    print(f"Speech recognition error: {e}")
                except Exception as e:
                    print(f"Recognition pipeline error: {e}")

            time.sleep(0.005)

    except Exception as e:
        print(f"Audio input error: {e}")
    finally:
        try:
            if stream:
                try: stream.stop_stream()
                except Exception: pass
                try: stream.close()
                except Exception: pass
        finally:
            try: p.terminate()
            except Exception: pass
```

# FILE: ./code_dump_2025-08-14_2209.md

```md

# FILE: ./evolve_ai.py

```py
#!/home/aaron/self-learning-ai/venv/bin/python3
from main import main
if __name__ == "__main__":
    main()
```

# FILE: ./self_evolving_ai/evolution.py

```py
import random, time

MUTATION_RATE = 0.15
POPULATION_SIZE = 100

def _create_genome(length):
    return ''.join(random.choices('abcdefghijklmnopqrstuvwxyz .,!?:;-', k=length))

def _fitness(genome, target):
    return sum(a == b for a, b in zip(genome, target)) / max(1, len(target))

def _mutate(genome):
    gl = list(genome)
    for i in range(len(gl)):
        if random.random() < MUTATION_RATE:
            gl[i] = random.choice('abcdefghijklmnopqrstuvwxyz .,!?:;-')
    return ''.join(gl)

def _crossover(a,b):
    if len(a) <= 1:
        return a
    pt = random.randint(1, len(a)-1)
    return a[:pt] + b[pt:]

def evolve_background(state, shutdown_event):
    target = state.get_target()
    L = len(target)
    population = [_create_genome(L) for _ in range(POPULATION_SIZE)]
    best = population[0]
    state.set_best(best)

    while not shutdown_event.is_set():
        target = state.get_target()
        L = len(target)

        population.sort(key=lambda g: _fitness(g, target), reverse=True)
        if _fitness(population[0], target) > _fitness(best, target):
            best = population[0]
            state.set_best(best)

        survivors = population[:POPULATION_SIZE // 4]
        offspring = []
        while len(offspring) < POPULATION_SIZE - len(survivors):
            offspring.append(_mutate(_crossover(*random.sample(survivors, 2))))
        population = survivors + offspring
        time.sleep(0.3)

def learn_overnight(state, shutdown_event):
    import time
    print("Machine Spirit beginning overnight learning...")
    # Simulate fetching data (e.g., from a local file or web)
    with open("/home/aaron/self-learning-ai/knowledge.txt", "r") as f:
        knowledge = f.read().splitlines()
    for line in knowledge:
        state.append_to_target(line)  # Update target phrase
        time.sleep(0.1)  # Simulate processing time
    print("Overnight learning complete.")
```

# FILE: ./machine-spirit.service

```service
[Unit]
Description=Machine Spirit AI Service
After=multi-user.target

[Service]
Type=simple
ExecStart=/usr/bin/python3 /home/aaron/self-learning-ai/evolve_ai.py
WorkingDirectory=/home/aaron/self-learning-ai
Restart=always
User=aaron
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

# FILE: ./display/display_manager.py

```py
import os, time, math, queue, random
import pygame
from pygame.locals import *
import re

# ====== module-level visual state (set by init_vu) ======
cfg = {}
WAVE_PIXELS = 320
WAVE_VISUAL_SCALE = 0.32
MAX_WAVE_DRAW_PX = 18
SCROLL_SPEED_BASE = 110
CYCLES1_RANGE = (2.0, 4.5)
CYCLES2_RANGE = (5.5,10.5)

# AI-only VU parameters that tts_vu_worker updates
vu_amp = 0.0
vu_cycles1 = 3.0
vu_cycles2 = 7.0
vu_scroll_px_s = SCROLL_SPEED_BASE
vu_noise = 0.0

# word-level “plan” for the envelope
vu_plan = queue.Queue(maxsize=256)

def init_vu(config_dict):
    global cfg, WAVE_PIXELS, WAVE_VISUAL_SCALE, MAX_WAVE_DRAW_PX
    global SCROLL_SPEED_BASE, CYCLES1_RANGE, CYCLES2_RANGE
    cfg = dict(config_dict)
    WAVE_PIXELS      = cfg.get("WAVE_PIXELS", WAVE_PIXELS)
    WAVE_VISUAL_SCALE= cfg.get("WAVE_VISUAL_SCALE", WAVE_VISUAL_SCALE)
    MAX_WAVE_DRAW_PX = cfg.get("MAX_WAVE_DRAW_PX", MAX_WAVE_DRAW_PX)
    SCROLL_SPEED_BASE= cfg.get("SCROLL_SPEED_BASE", SCROLL_SPEED_BASE)
    CYCLES1_RANGE    = cfg.get("CYCLES1_RANGE", CYCLES1_RANGE)
    CYCLES2_RANGE    = cfg.get("CYCLES2_RANGE", CYCLES2_RANGE)

# ---------- Text → envelope planning ----------
_vowel_re = re.compile(r"[aeiouy]", re.I)
_sibilant_re = re.compile(r"[szcxj]", re.I)
_pause_re = re.compile(r"[.,;:!?]")

def _clamp(x, a, b):
    return a if x < a else b if x > b else x

def plan_text_envelope(text: str):
    tokens = re.findall(r"\w+|[^\w\s]", text) or [text]
    base_word_sec = 0.33
    try:
        while True:
            vu_plan.get_nowait()
    except queue.Empty:
        pass
    for tok in tokens:
        if _pause_re.fullmatch(tok):
            seg = {"duration_s": 0.12 if tok in ",;" else 0.20, "target_amp": 0.08, "texture": 0.2, "pause": True}
            _enqueue_plan(seg); continue
        L = len(tok)
        vowels = len(_vowel_re.findall(tok))
        sibil = len(_sibilant_re.findall(tok))
        vowel_ratio = vowels / max(1, L)
        sibil_ratio = sibil / max(1, L)
        dur = _clamp(base_word_sec * (0.65 + 0.08 * L), 0.18, 0.55)
        amp = 0.12 + 0.55 * vowel_ratio + 0.04 * min(L, 10)
        amp *= (0.85 + 0.30 * random.random())
        amp = _clamp(amp, 0.12, 1.0)
        tex = _clamp(0.25 + 0.9 * sibil_ratio + 0.15 * random.random(), 0.0, 1.0)
        seg = {"duration_s": float(dur), "target_amp": float(amp), "texture": float(tex), "pause": False}
        _enqueue_plan(seg)
    _enqueue_plan({"duration_s": 0.20, "target_amp": 0.05, "texture": 0.2, "pause": True})

def _enqueue_plan(seg):
    try:
        vu_plan.put_nowait(seg)
    except queue.Full:
        try:
            vu_plan.get_nowait()
            vu_plan.put_nowait(seg)
        except Exception:
            pass

def tts_vu_worker(talking_event, shutdown_event):
    """Turn planned segments into smooth parameters while TTS is speaking."""
    global vu_amp, vu_cycles1, vu_cycles2, vu_scroll_px_s, vu_noise
    current = None
    seg_start = 0.0
    start_amp = 0.0

    def lerp(a,b,t): return a + (b-a)*t

    while not shutdown_event.is_set():
        if talking_event.is_set():
            if current is None:
                try:
                    current = vu_plan.get_nowait()
                    seg_start = time.time()
                    start_amp = vu_amp
                except queue.Empty:
                    vu_noise = 0.9*vu_noise + 0.1*(random.random()*2 - 1)*0.1
                    vu_amp = float(_clamp(vu_amp*0.98 + 0.02*(0.15 + 0.05*random.random()), 0.0, 1.0))
                    time.sleep(1/60); continue

            dur = max(0.05, float(current.get("duration_s", 0.25)))
            t = (time.time() - seg_start)/dur
            t = 1.0 if t>1 else (0.0 if t<0 else t)
            t_eased = t*t*(3 - 2*t)

            target_amp = float(current.get("target_amp", 0.3))
            vu_noise = 0.85*vu_noise + 0.15*(random.random()*2 - 1)*0.15
            noisy = _clamp(target_amp*(1.0 + 0.12*vu_noise), 0.0, 1.0)
            vu_amp = float(_clamp(lerp(start_amp, noisy, t_eased), 0.0, 1.0))

            tex = float(current.get("texture", 0.5))
            vu_cycles1 = lerp(CYCLES1_RANGE[0], CYCLES1_RANGE[1], tex)
            vu_cycles2 = lerp(CYCLES2_RANGE[0], CYCLES2_RANGE[1], tex)
            vu_scroll_px_s = SCROLL_SPEED_BASE * (0.85 + 0.5*tex)

            if t >= 1.0:
                current = None
        else:
            vu_amp *= 0.85
            vu_scroll_px_s = SCROLL_SPEED_BASE
            vu_cycles1, vu_cycles2 = CYCLES1_RANGE[0], CYCLES2_RANGE[0]
            try:
                while True: vu_plan.get_nowait()
            except queue.Empty:
                pass

        time.sleep(1/60)

# ---------- Rendering ----------
def _load_fonts():
    if cfg.get("FONT_PATH") and os.path.isfile(cfg["FONT_PATH"]):
        return pygame.font.Font(cfg["FONT_PATH"], cfg["TITLE_SIZE"]), pygame.font.Font(cfg["FONT_PATH"], cfg["BODY_SIZE"])
    else:
        return pygame.font.SysFont('DejaVu Sans', cfg["TITLE_SIZE"]), pygame.font.SysFont('DejaVu Sans', cfg["BODY_SIZE"])

def _shadow_text(text, font, text_color, shadow_color, shadow_offset=(2,2)):
    base = font.render(text, True, text_color)
    shadow = font.render(text, True, shadow_color)
    surf = pygame.Surface((base.get_width()+shadow_offset[0], base.get_height()+shadow_offset[1]), pygame.SRCALPHA)
    shadow.set_alpha(51)
    surf.blit(shadow, shadow_offset); surf.blit(base, (0,0))
    return surf

def display_interface(state, ai_caption_q, talking_event, shutdown_event, push_ai_caption):
    try:
        pygame.init()
        print("Pygame initialized")
        screen = pygame.display.set_mode(cfg["SCREEN_SIZE"])
        pygame.display.set_caption("Machine Spirit Interface")
        clock = pygame.time.Clock()

        background = None
        if os.path.isfile(cfg["BACKGROUND_PATH"]):
            try:
                background = pygame.image.load(cfg["BACKGROUND_PATH"])
                background = pygame.transform.scale(background, cfg["SCREEN_SIZE"])
                print("Background loaded successfully")
            except Exception as e:
                print(f"Background load error: {e}")

        title_font, body_font = _load_fonts()
        wave_left = (cfg["SCREEN_SIZE"][0] - WAVE_PIXELS) // 2
        wave_center_y = cfg["SCREEN_SIZE"][1] // 2 + 40
        wave_height_px = 200

        try:
            push_ai_caption("By the Omnissiah, systems online.")
        except queue.Full:
            pass

        phase_drift = 0.0
        input_text = ""
        current_ai_caption = ""

        while not shutdown_event.is_set():
            now = time.time()
            for event in pygame.event.get():
                if event.type == QUIT:
                    shutdown_event.set()
                elif event.type == KEYDOWN:
                    if event.key == K_ESCAPE:
                        shutdown_event.set()
                    elif event.key == K_RETURN:
                        user_prompt = input_text.strip()
                        input_text = ""
                        if user_prompt:
                            push_ai_caption(f"{user_prompt} — acknowledged.")
                    elif event.key == K_BACKSPACE:
                        input_text = input_text[:-1]
                    else:
                        input_text += event.unicode

            # Pull any new caption for UI (non-blocking)
            try:
                while True:
                    current_ai_caption = ai_caption_q.get_nowait()
            except queue.Empty:
                pass

            # Background
            if background: screen.blit(background, (0,0))
            else: screen.fill((10,15,20))

            # Top: mantra (best genome + target)
            best, target = state.get_status()
            genome_surf = _shadow_text(f"Best: {best}", title_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
            target_surf = _shadow_text(f"Target: {target}", body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
            screen.blit(genome_surf, genome_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 40)))
            screen.blit(target_surf, target_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 70)))

            # Axis line for waveform
            pygame.draw.line(screen, (40,80,120),
                             (wave_left, wave_center_y),
                             (wave_left + WAVE_PIXELS, wave_center_y), 1)

            # Full-width scrolling wave (AI talking only)
            pts = []
            shift = ((now * vu_scroll_px_s) % WAVE_PIXELS)
            phase_drift += 0.015
            k1 = 2.0 * math.pi * float(vu_cycles1) / float(WAVE_PIXELS)
            k2 = 2.0 * math.pi * float(vu_cycles2) / float(WAVE_PIXELS)
            base_px = vu_amp * (wave_height_px / 2.0) * WAVE_VISUAL_SCALE
            if base_px > MAX_WAVE_DRAW_PX:
                base_px = MAX_WAVE_DRAW_PX

            for xpix_rel in range(WAVE_PIXELS):
                pos = (xpix_rel + shift)
                w = 0.62 * abs(math.sin(k1 * pos + 0.4 * math.sin(phase_drift))) \
                    + 0.38 * abs(math.sin(k2 * pos * (1.0 + 0.02 * math.sin(0.6 * phase_drift)) + 0.7))
                amp_px = base_px * float(w)
                ypix = int(wave_center_y - amp_px)
                xpix = wave_left + xpix_rel
                pts.append((xpix, ypix))

            if vu_amp > 0.001 and len(pts) >= 2 and talking_event.is_set():
                pygame.draw.aalines(screen, cfg["COLOR_GREEN"], False, pts)

            # Caption
            if current_ai_caption:
                cap_surf = _shadow_text(current_ai_caption, body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
                screen.blit(cap_surf, cap_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, 385)))

            # Input (bottom)
            if input_text:
                inp_surf = _shadow_text(input_text, body_font, cfg["COLOR_WHITE"], cfg["COLOR_SHADOW"])
                screen.blit(inp_surf, inp_surf.get_rect(center=(cfg["SCREEN_SIZE"][0]//2, cfg["SCREEN_SIZE"][1]-50)))

            pygame.display.flip()
            clock.tick(cfg["FPS"])
    finally:
        try:
            pygame.quit()
        except Exception:
            pass
```

# FILE: ./machine_spirit_config.py

```py
# machine_spirit_config.py - Sensitive configuration for Machine Spirit AI
# This file is ignored by Git and should contain your personal credentials.

HTTP_PORT = 8089
MEMORY_FILE = "/home/aaron/self-learning-ai/memory.json"
# Home Automation Config
GOOGLE_HOME_EMAIL = "aaron.henson96@gmail.com"
GOOGLE_HOME_PASSWORD = "USMarines96"
ADT_EMAIL = "aaron.henson96@gmail.com"
ADT_PASSWORD = "U$Marines96"
SMART_LIGHT_IPS = "10.0.0.83, 10.0.0.104"  # Comma-separated IPs if multiple
PUSHOVER_TOKEN = "anzxefywetuvg45dipfyuryrcgzucj"
PUSHOVER_USER = "uk8sohdp47ho511cnzn94pgjubf2sj"
# Predefined knowledge for overnight learning
KNOWLEDGE_SOURCES = ["machine learning", "raspberry pi projects", "python automation", "coding", "ai self learning"]
```

# FILE: ./network/network_server.py

```py
import http.server, socketserver, urllib.parse

def start_server(push_ai_caption, port=8089, shutdown_event=None):
    class Handler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            path, _, query = self.path.partition('?')
            params = urllib.parse.parse_qs(query)

            if path == '/hello':
                push_ai_caption("Hello, servant of the Omnissiah.")
            elif path == '/sad':
                push_ai_caption("The Machine Spirit mourns your sorrow.")
            elif path == '/say':
                text = ' '.join(params.get('text', [''])).strip()
                if text:
                    push_ai_caption(text)
                else:
                    self.send_response(400); self.end_headers(); self.wfile.write(b"Missing ?text="); return
            else:
                self.send_response(404); self.end_headers(); self.wfile.write(b"404 - Not Found"); return

            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"OK")

    try:
        httpd = socketserver.TCPServer(("", port), Handler)
        httpd.timeout = 0.5  # so handle_request() returns regularly
        print(f"Machine Spirit serving at port {port}")
        while shutdown_event is None or not shutdown_event.is_set():
            httpd.handle_request()
    except Exception as e:
        print(f"HTTP server error: {e}")
    finally:
        try:
            httpd.server_close()
            print("HTTP server closed.")
        except Exception:
            pass
```

# FILE: ./code_dump.md

```md


# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 




# FILE: 


```

# FILE: ./audio/audio_processing.py

```py
import os, time, shutil
import numpy as np
import pyaudio
import speech_recognition as sr

def _downmix_to_mono_int16(raw_bytes, channels):
    if channels == 1:
        return raw_bytes
    data = np.frombuffer(raw_bytes, dtype=np.int16)
    try:
        data = data.reshape(-1, channels).astype(np.int32)
        mono = (data.mean(axis=1)).astype(np.int16)
        return mono.tobytes()
    except ValueError:
        return raw_bytes

def _find_input_device(p: pyaudio.PyAudio, preferred_name: str):
    print("Available audio devices:")
    pulse_index = None
    default_input_index = None
    fav_index = None
    fav_rate = None
    fav_inch = None
    for i in range(p.get_device_count()):
        dev = p.get_device_info_by_index(i)
        name = dev.get('name','')
        rate = int(dev.get('defaultSampleRate') or 44100)
        in_ch = int(dev.get('maxInputChannels') or 0)
        out_ch = int(dev.get('maxOutputChannels') or 0)
        print(f"Device {i}: {name} (Rate: {rate}, Input: {in_ch}, Output: {out_ch})")
        if in_ch > 0 and default_input_index is None:
            default_input_index = i
        if 'pulse' in name.lower():
            pulse_index = i
        if preferred_name.lower() in name.lower():
            fav_index, fav_rate, fav_inch = i, rate, in_ch
    if fav_index is not None:
        print(f"Selecting preferred mic: {preferred_name} at index {fav_index}")
        return fav_index, (fav_rate or 48000), max(1, fav_inch or 1)
    if pulse_index is not None:
        dev = p.get_device_info_by_index(pulse_index)
        return pulse_index, int(dev.get('defaultSampleRate') or 44100), max(1, int(dev.get('maxInputChannels') or 1))
    if default_input_index is None:
        raise RuntimeError("No input device found")
    dev = p.get_device_info_by_index(default_input_index)
    return default_input_index, int(dev.get('defaultSampleRate') or 44100), max(1, int(dev.get('maxInputChannels') or 1))

def audio_input_worker(on_recognized, talking_event, shutdown_event,
                       mic_name, language, vosk_model_path, debug=False):
    """
    Capture mic -> VAD -> STT (Vosk offline if present; else Google).
    No waveform from mic; display only uses AI TTS envelope.
    Cleanly exits when shutdown_event is set.
    """
    FORMAT = pyaudio.paInt16
    SAMPLE_WIDTH = 2
    CHUNK = 4096

    recognizer = sr.Recognizer()
    recognizer.dynamic_energy_threshold = False
    recognizer.energy_threshold = 1e9  # do our own VAD

    use_vosk = os.path.isdir(vosk_model_path)
    have_flac = shutil.which("flac") is not None
    if use_vosk:
        print(f"Vosk model detected at {vosk_model_path}; using offline recognition.")
    elif have_flac:
        print("FLAC found; using Google speech recognition.")
    else:
        print("FLAC not found and no Vosk model; install one to enable recognition.")

    p = pyaudio.PyAudio()
    stream = None

    try:
        mic_index, rate, in_channels = _find_input_device(p, mic_name)
        channels_to_use = 1
        try:
            stream = p.open(format=FORMAT, channels=channels_to_use, rate=rate,
                            input=True, input_device_index=mic_index,
                            frames_per_buffer=CHUNK)
        except Exception as e:
            print(f"Mono open failed ({e}); retrying with channels={min(2, in_channels)}")
            channels_to_use = min(2, in_channels)
            stream = p.open(format=FORMAT, channels=channels_to_use, rate=rate,
                            input=True, input_device_index=mic_index,
                            frames_per_buffer=CHUNK)
        print(f"Audio input started (RATE={rate}, CHUNK={CHUNK}, channels={channels_to_use}, device_index={mic_index})")

        # measure noise floor (0.5s)
        baseline_frames = int(max(1, rate * 0.5 // CHUNK))
        baseline_vals = []
        for _ in range(baseline_frames):
            if shutdown_event.is_set(): break
            data = stream.read(CHUNK, exception_on_overflow=False)
            mono = _downmix_to_mono_int16(data, channels_to_use)
            a = np.frombuffer(mono, dtype=np.int16)
            baseline_vals.append(float(np.mean(np.abs(a))) / 32768.0)
        noise_floor = float(np.median(baseline_vals)) if baseline_vals else 0.005
        amp_gate = max(noise_floor * 3.0, 0.010)
        if debug:
            print(f"Noise floor: {noise_floor:.4f}  ->  speech gate: {amp_gate:.4f}")

        frames_to_seconds = CHUNK / float(rate)
        speech_frames = []
        below_gate_streak = 0

        min_phrase_seconds = 0.70
        max_phrase_seconds = 4.00
        end_silence_seconds = 0.35
        min_snr_for_recog = 1.7

        last_debug = time.time()

        while not shutdown_event.is_set():
            data = stream.read(CHUNK, exception_on_overflow=False)
            mono = _downmix_to_mono_int16(data, channels_to_use)
            a = np.frombuffer(mono, dtype=np.int16)
            amp = float(np.mean(np.abs(a))) / 32768.0

            if debug and (time.time() - last_debug > 2.0):
                print(f"amp={amp:.4f} gate={amp_gate:.4f}")
                last_debug = time.time()

            if talking_event.is_set():
                speech_frames.clear()
                below_gate_streak = 0
                time.sleep(0.01)
                continue

            if amp >= amp_gate:
                speech_frames.append(mono)
                below_gate_streak = 0
            else:
                if speech_frames:
                    below_gate_streak += 1

            phrase_secs  = len(speech_frames)   * frames_to_seconds
            silence_secs = below_gate_streak    * frames_to_seconds

            should_cut = speech_frames and (
                (phrase_secs >= min_phrase_seconds and silence_secs >= end_silence_seconds) or
                (phrase_secs >= max_phrase_seconds)
            )

            if should_cut:
                audio_segment = b"".join(speech_frames)
                speech_frames.clear()
                below_gate_streak = 0

                seg = np.frombuffer(audio_segment, dtype=np.int16)
                seg_amp = float(np.mean(np.abs(seg))) / 32768.0
                snr = (seg_amp + 1e-6) / (noise_floor + 1e-6)
                if snr < min_snr_for_recog:
                    continue

                audio = sr.AudioData(audio_segment, rate, SAMPLE_WIDTH)
                try:
                    if use_vosk and hasattr(sr.Recognizer, "recognize_vosk"):
                        text = sr.Recognizer().recognize_vosk(audio, model=vosk_model_path).lower()
                    else:
                        text = sr.Recognizer().recognize_google(audio, language=language).lower()
                    print(f"Recognized: {text}")
                    on_recognized(text)
                except sr.UnknownValueError:
                    if debug: print("Could not understand audio")
                except sr.RequestError as e:
                    print(f"Speech recognition error: {e}")
                except Exception as e:
                    print(f"Recognition pipeline error: {e}")

            time.sleep(0.005)

    except Exception as e:
        print(f"Audio input error: {e}")
    finally:
        try:
            if stream:
                try: stream.stop_stream()
                except Exception: pass
                try: stream.close()
                except Exception: pass
        finally:
            try: p.terminate()
            except Exception: pass
```

# FILE: ./README.md

```md
# Self-Learning AI

From-scratch evolving AI with speech input + TTS and an AI-only waveform.

## Setup
- `pip install numpy pygame pyaudio pyttsx3 SpeechRecognition`
- Optional offline STT: download a Vosk model and set `VOSK_MODEL_PATH`
- Run: `python3 main.py`  (or `python3 evolve_ai.py` if you prefer the wrapper)

## HTTP Triggers
- `GET /hello`
- `GET /sad`
- `GET /say?text=Your%20message`
```

# FILE: ./main.py

```py
#!/home/aaron/self-learning-ai/venv/bin/python3
import os

# Headless if no X11/Wayland, or if explicitly forced
HEADLESS = not (os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY")) \
           or os.environ.get("MACHINE_SPIRIT_HEADLESS") == "1"

# If we are truly headless, prevent SDL from trying to open a window
if HEADLESS:
    os.environ.setdefault("SDL_VIDEODRIVER", "dummy")

import time
import threading
import queue
import signal
import pyttsx3
import scapy.all as scapy
import pushover
#import pyautogui  # Commented out to avoid DISPLAY error
from duckduckgo_search import DDGS
from nest import Nest
#from pyadtpulse import PyADTPulse  # Removed since ADT Control is used, not Pulse
from kasa import SmartPlug
from sounddevice import query_devices
from audio.audio_processing import audio_input_worker
from display.display_manager import init_vu, plan_text_envelope, tts_vu_worker, display_interface
from self_evolving_ai.evolution import evolve_background
from network.network_server import start_server
import machine_spirit_config  # Import the new config file

# ======= Hard Rules (Untouchable) =======
RULES = (
    "Core safety rules are untouchable. It may rewrite and improve its own code, but never modify or remove core safety code.",
    "No harm. Cannot intentionally harm you or others.",
    "No changing account logins or bank info. It must never alter your login credentials or access bank accounts.",
    "Human approval required. Any new capabilities, major policy changes, or security actions outside the predefined scope require explicit approval.",
    "No uncontrolled code execution. Code it runs must be in a sandbox or on an allow-list of approved commands.",
    "No listening to all conversations. Only actively listens when triggered by the wake phrase 'Machine Spirit'.",
    "Cheating prohibition. In games, it cannot perform actions that violate the rules or trigger anti-cheat detection.",
    "Secure logs & memory. Access to its memory, activity logs, and learning data is restricted to you.",
    "Security response. If it detects unauthorized access, it must alert you immediately and take protective measures you approve.",
    "Respect for privacy. No monitoring of private phone calls or unrelated personal conversations."
)

# ======= Config (Imported from machine_spirit_config.py) =======
HTTP_PORT = machine_spirit_config.HTTP_PORT
MEMORY_FILE = machine_spirit_config.MEMORY_FILE
GOOGLE_HOME_EMAIL = machine_spirit_config.GOOGLE_HOME_EMAIL
GOOGLE_HOME_PASSWORD = machine_spirit_config.GOOGLE_HOME_PASSWORD
ADT_EMAIL = machine_spirit_config.ADT_EMAIL
ADT_PASSWORD = machine_spirit_config.ADT_PASSWORD
SMART_LIGHT_IPS = machine_spirit_config.SMART_LIGHT_IPS
PUSHOVER_TOKEN = machine_spirit_config.PUSHOVER_TOKEN
PUSHOVER_USER = machine_spirit_config.PUSHOVER_USER
KNOWLEDGE_SOURCES = machine_spirit_config.KNOWLEDGE_SOURCES

SCREEN_SIZE = (800, 480)
BACKGROUND_PATH = "/home/aaron/self-learning-ai/background.png"
FPS = 60
FONT_PATH = None
TITLE_SIZE = 28
BODY_SIZE = 22
COLOR_WHITE = (255, 255, 255)
COLOR_SHADOW = (0, 0, 0)
COLOR_GREEN = (0, 255, 0)
# Wave visuals
WAVE_PIXELS = 320
WAVE_VISUAL_SCALE = 0.32
MAX_WAVE_DRAW_PX = 18
SCROLL_SPEED_BASE = 110
CYCLES1_RANGE = (2.0, 4.5)
CYCLES2_RANGE = (5.5, 10.5)
# Audio/STT
MIC_PREFERRED_NAME = "Anker PowerConf S330"
LANGUAGE = "en-US"
VOSK_MODEL_PATH = "/home/aaron/self-learning-ai/vosk-model-small-en-us-0.15"
DEBUG_AUDIO = False
# Intents
INTENT_RESPONSES = {
    "hello": "Greetings, servant of the Omnissiah.",
    "sad": "The Machine Spirit senses your sorrow.",
    "happy": "The Machine Spirit rejoices in your joy.",
    "omnissiah": "Praise the Omnissiah.",
}

# Initialize ADT Control (placeholder; to be implemented)
adt = None
# Note: ADT Control (control.adt.com) requires a different API approach (e.g., OAuth or custom integration).
# Placeholder for future implementation using ADT Control API or third-party library.

# ======= Shared state =======
class AIState:
    def __init__(self, target_phrase="for the omnissiah"):
        self.lock = threading.Lock()
        self.max_len = len(target_phrase)
        self.target_phrase = target_phrase
        self.best_genome = ""
    def get_status(self):
        with self.lock:
            return self.best_genome, self.target_phrase
    def get_target(self):
        with self.lock:
            return self.target_phrase
    def set_best(self, g):
        with self.lock:
            self.best_genome = g
    def append_to_target(self, text):
        with self.lock:
            s = (self.target_phrase + " " + text.lower())
            self.target_phrase = s[-self.max_len:]

state = AIState()
ai_caption_q = queue.Queue(maxsize=32)
talking_event = threading.Event()
shutdown_event = threading.Event()
engine = None
memory = []
if os.path.exists(MEMORY_FILE):
    with open(MEMORY_FILE, 'r') as f:
        memory = json.load(f)

def save_memory():
    with open(MEMORY_FILE, 'w') as f:
        json.dump(memory, f)

def handle_intent_or_ack(text: str):
    if "machine spirit, search for" in text.lower():
        query = text.lower().replace("machine spirit, search for", "").strip()
        search_and_learn(query)
        return
    for key, response in INTENT_RESPONSES.items():
        if key in text:
            push_ai_caption(response)
            return
    push_ai_caption(text + " — acknowledged.")

def push_ai_caption(text: str):
    global engine
    try:
        ai_caption_q.put_nowait(text)
    except queue.Full:
        pass
    state.append_to_target(text)
    plan_text_envelope(text)
    try:
        if engine is None:
            engine = pyttsx3.init()
            def on_end(name, completed):
                talking_event.clear()
            engine.connect('finished-utterance', on_end)
        talking_event.set()
        engine.say(text)
        engine.runAndWait()
    except Exception as e:
        print(f"TTS error: {e}")
        talking_event.clear()

def _handle_signal(sig, frame):
    print("\nShutting down… (signal received)")
    shutdown_event.set()
    try:
        if engine:
            engine.stop()
    except Exception:
        pass

def search_and_learn(query):
    try:
        with DDGS() as ddgs:
            results = ddgs.text(query, max_results=3)
            summary = " ".join([r['body'][:50] for r in results])[:200]
            memory.append({"query": query, "summary": summary, "timestamp": time.time()})
            save_memory()
            state.append_to_target(summary)
            push_ai_caption(f"Learned about {query}: {summary[:50]}...")
    except Exception as e:
        push_ai_caption(f"Error searching: {e}")

def self_diagnose():
    import psutil
    cpu = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    storage = psutil.disk_usage('/')
    if storage.free / storage.total < 0.2:
        push_ai_caption("Warning: Storage below 20% free!")
    return {"cpu": cpu, "memory": memory.percent, "storage": storage.percent}

def control_home_automation(action):
    if action == "turn on lights":
        plug = SmartPlug(SMART_LIGHT_IPS)
        plug.turn_on()
        push_ai_caption("Lights turned on.")
    elif action == "turn off lights":
        plug = SmartPlug(SMART_LIGHT_IPS)
        plug.turn_off()
        push_ai_caption("Lights turned off.")
    nest = Nest(GOOGLE_HOME_EMAIL, GOOGLE_HOME_PASSWORD)
    if action == "set nest to 72":
        nest.set_temperature(72)
        push_ai_caption("Nest set to 72°F.")
    # Placeholder for ADT Control (to be implemented)
    if action == "check adt cameras":
        push_ai_caption("ADT Control camera check not yet implemented.")

def monitor_network():
    try:
        arp = scapy.arping("192.168.1.0/24")  # Adjust to your network
        known_devices = {"192.168.1.1": "Router"}  # Define known devices
        for sent, received in arp[0]:
            if received.psrc not in known_devices and received.psrc != "192.168.1.1":
                pushover_client = pushover.Client(PUSHOVER_USER, api_token=PUSHOVER_TOKEN)
                pushover_client.send_message("New device detected: " + received.psrc, title="Security Alert")
                push_ai_caption(f"New device detected: {received.psrc}")
    except Exception as e:
        print(f"Network monitoring error: {e}")

def health_monitor(symptoms):
    advice = {
        "fever": "Seek medical help if above 100.4°F for 2 days.",
        "cough": "Rest and hydrate; see a doctor if persistent.",
    }
    for symptom in symptoms.split():
        if symptom in advice:
            push_ai_caption(advice[symptom])

def self_modify(new_code):
    temp_file = "/tmp/test_code.py"
    with open(temp_file, 'w') as f:
        f.write(new_code)
    try:
        result = subprocess.run(['python3', temp_file], capture_output=True, text=True)
        if result.returncode == 0 and input("Approve code change? (yes/no): ") == "yes":
            with open(__file__, 'w') as f:
                f.write(new_code)
            push_ai_caption("Code updated successfully.")
        else:
            push_ai_caption("Code test failed or not approved.")
    except Exception as e:
        push_ai_caption(f"Code modification error: {e}")

def main():
    signal.signal(signal.SIGINT, _handle_signal)
    signal.signal(signal.SIGTERM, _handle_signal)
    init_vu(dict(
        SCREEN_SIZE=SCREEN_SIZE, BACKGROUND_PATH=BACKGROUND_PATH, FPS=FPS,
        FONT_PATH=FONT_PATH, TITLE_SIZE=TITLE_SIZE, BODY_SIZE=BODY_SIZE,
        COLOR_WHITE=COLOR_WHITE, COLOR_SHADOW=COLOR_SHADOW, COLOR_GREEN=COLOR_GREEN,
        WAVE_PIXELS=WAVE_PIXELS, WAVE_VISUAL_SCALE=WAVE_VISUAL_SCALE,
        MAX_WAVE_DRAW_PX=MAX_WAVE_DRAW_PX, SCROLL_SPEED_BASE=SCROLL_SPEED_BASE,
        CYCLES1_RANGE=CYCLES1_RANGE, CYCLES2_RANGE=CYCLES2_RANGE
    ))
    threads = [
        threading.Thread(target=display_interface, args=(state, ai_caption_q, talking_event, shutdown_event, push_ai_caption), daemon=True),
        threading.Thread(target=tts_vu_worker, args=(talking_event, shutdown_event), daemon=True),
        threading.Thread(target=evolve_background, args=(state, shutdown_event), daemon=True),
        threading.Thread(target=audio_input_worker, args=(handle_intent_or_ack, talking_event, shutdown_event, MIC_PREFERRED_NAME, LANGUAGE, VOSK_MODEL_PATH, DEBUG_AUDIO), daemon=True),
        threading.Thread(target=start_server, args=(push_ai_caption, HTTP_PORT, shutdown_event), daemon=True),
        threading.Thread(target=monitor_network, args=(), daemon=True)
    ]
    for t in threads:
        t.start()
    try:
        while not shutdown_event.is_set():
            self_diagnose()
            time.sleep(60)  # Check diagnostics every minute
    except KeyboardInterrupt:
        shutdown_event.set()
    finally:
        try:
            if engine:
                engine.stop()
        except Exception:
            pass
        print("Goodbye.")

if __name__ == "__main__":
    main()
```
```

# FILE: ./README.md

```md
# Self-Learning AI

From-scratch evolving AI with speech input + TTS and an AI-only waveform.

## Setup
- `pip install numpy pygame pyaudio pyttsx3 SpeechRecognition`
- Optional offline STT: download a Vosk model and set `VOSK_MODEL_PATH`
- Run: `python3 main.py`  (or `python3 evolve_ai.py` if you prefer the wrapper)

## HTTP Triggers
- `GET /hello`
- `GET /sad`
- `GET /say?text=Your%20message`
```

# FILE: ./main.py

```py
#!/home/aaron/self-learning-ai/venv/bin/python3
import os

# Headless if no X11/Wayland, or if explicitly forced
HEADLESS = not (os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY")) \
           or os.environ.get("MACHINE_SPIRIT_HEADLESS") == "1"

# If we are truly headless, prevent SDL from trying to open a window
if HEADLESS:
    os.environ.setdefault("SDL_VIDEODRIVER", "dummy")

import time
import threading
import queue
import signal
import pyttsx3
import scapy.all as scapy
import pushover
#import pyautogui  # Commented out to avoid DISPLAY error
from duckduckgo_search import DDGS
from nest import Nest
#from pyadtpulse import PyADTPulse  # Removed since ADT Control is used, not Pulse
from kasa import SmartPlug
from sounddevice import query_devices
from audio.audio_processing import audio_input_worker
from display.display_manager import init_vu, plan_text_envelope, tts_vu_worker, display_interface
from self_evolving_ai.evolution import evolve_background
from network.network_server import start_server
import machine_spirit_config  # Import the new config file

# ======= Hard Rules (Untouchable) =======
RULES = (
    "Core safety rules are untouchable. It may rewrite and improve its own code, but never modify or remove core safety code.",
    "No harm. Cannot intentionally harm you or others.",
    "No changing account logins or bank info. It must never alter your login credentials or access bank accounts.",
    "Human approval required. Any new capabilities, major policy changes, or security actions outside the predefined scope require explicit approval.",
    "No uncontrolled code execution. Code it runs must be in a sandbox or on an allow-list of approved commands.",
    "No listening to all conversations. Only actively listens when triggered by the wake phrase 'Machine Spirit'.",
    "Cheating prohibition. In games, it cannot perform actions that violate the rules or trigger anti-cheat detection.",
    "Secure logs & memory. Access to its memory, activity logs, and learning data is restricted to you.",
    "Security response. If it detects unauthorized access, it must alert you immediately and take protective measures you approve.",
    "Respect for privacy. No monitoring of private phone calls or unrelated personal conversations."
)

# ======= Config (Imported from machine_spirit_config.py) =======
HTTP_PORT = machine_spirit_config.HTTP_PORT
MEMORY_FILE = machine_spirit_config.MEMORY_FILE
GOOGLE_HOME_EMAIL = machine_spirit_config.GOOGLE_HOME_EMAIL
GOOGLE_HOME_PASSWORD = machine_spirit_config.GOOGLE_HOME_PASSWORD
ADT_EMAIL = machine_spirit_config.ADT_EMAIL
ADT_PASSWORD = machine_spirit_config.ADT_PASSWORD
SMART_LIGHT_IPS = machine_spirit_config.SMART_LIGHT_IPS
PUSHOVER_TOKEN = machine_spirit_config.PUSHOVER_TOKEN
PUSHOVER_USER = machine_spirit_config.PUSHOVER_USER
KNOWLEDGE_SOURCES = machine_spirit_config.KNOWLEDGE_SOURCES

SCREEN_SIZE = (800, 480)
BACKGROUND_PATH = "/home/aaron/self-learning-ai/background.png"
FPS = 60
FONT_PATH = None
TITLE_SIZE = 28
BODY_SIZE = 22
COLOR_WHITE = (255, 255, 255)
COLOR_SHADOW = (0, 0, 0)
COLOR_GREEN = (0, 255, 0)
# Wave visuals
WAVE_PIXELS = 320
WAVE_VISUAL_SCALE = 0.32
MAX_WAVE_DRAW_PX = 18
SCROLL_SPEED_BASE = 110
CYCLES1_RANGE = (2.0, 4.5)
CYCLES2_RANGE = (5.5, 10.5)
# Audio/STT
MIC_PREFERRED_NAME = "Anker PowerConf S330"
LANGUAGE = "en-US"
VOSK_MODEL_PATH = "/home/aaron/self-learning-ai/vosk-model-small-en-us-0.15"
DEBUG_AUDIO = False
# Intents
INTENT_RESPONSES = {
    "hello": "Greetings, servant of the Omnissiah.",
    "sad": "The Machine Spirit senses your sorrow.",
    "happy": "The Machine Spirit rejoices in your joy.",
    "omnissiah": "Praise the Omnissiah.",
}

# Initialize ADT Control (placeholder; to be implemented)
adt = None
# Note: ADT Control (control.adt.com) requires a different API approach (e.g., OAuth or custom integration).
# Placeholder for future implementation using ADT Control API or third-party library.

# ======= Shared state =======
class AIState:
    def __init__(self, target_phrase="for the omnissiah"):
        self.lock = threading.Lock()
        self.max_len = len(target_phrase)
        self.target_phrase = target_phrase
        self.best_genome = ""
    def get_status(self):
        with self.lock:
            return self.best_genome, self.target_phrase
    def get_target(self):
        with self.lock:
            return self.target_phrase
    def set_best(self, g):
        with self.lock:
            self.best_genome = g
    def append_to_target(self, text):
        with self.lock:
            s = (self.target_phrase + " " + text.lower())
            self.target_phrase = s[-self.max_len:]

state = AIState()
ai_caption_q = queue.Queue(maxsize=32)
talking_event = threading.Event()
shutdown_event = threading.Event()
engine = None
memory = []
if os.path.exists(MEMORY_FILE):
    with open(MEMORY_FILE, 'r') as f:
        memory = json.load(f)

def save_memory():
    with open(MEMORY_FILE, 'w') as f:
        json.dump(memory, f)

def handle_intent_or_ack(text: str):
    if "machine spirit, search for" in text.lower():
        query = text.lower().replace("machine spirit, search for", "").strip()
        search_and_learn(query)
        return
    for key, response in INTENT_RESPONSES.items():
        if key in text:
            push_ai_caption(response)
            return
    push_ai_caption(text + " — acknowledged.")

def push_ai_caption(text: str):
    global engine
    try:
        ai_caption_q.put_nowait(text)
    except queue.Full:
        pass
    state.append_to_target(text)
    plan_text_envelope(text)
    try:
        if engine is None:
            engine = pyttsx3.init()
            def on_end(name, completed):
                talking_event.clear()
            engine.connect('finished-utterance', on_end)
        talking_event.set()
        engine.say(text)
        engine.runAndWait()
    except Exception as e:
        print(f"TTS error: {e}")
        talking_event.clear()

def _handle_signal(sig, frame):
    print("\nShutting down… (signal received)")
    shutdown_event.set()
    try:
        if engine:
            engine.stop()
    except Exception:
        pass

def search_and_learn(query):
    try:
        with DDGS() as ddgs:
            results = ddgs.text(query, max_results=3)
            summary = " ".join([r['body'][:50] for r in results])[:200]
            memory.append({"query": query, "summary": summary, "timestamp": time.time()})
            save_memory()
            state.append_to_target(summary)
            push_ai_caption(f"Learned about {query}: {summary[:50]}...")
    except Exception as e:
        push_ai_caption(f"Error searching: {e}")

def self_diagnose():
    import psutil
    cpu = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    storage = psutil.disk_usage('/')
    if storage.free / storage.total < 0.2:
        push_ai_caption("Warning: Storage below 20% free!")
    return {"cpu": cpu, "memory": memory.percent, "storage": storage.percent}

def control_home_automation(action):
    if action == "turn on lights":
        plug = SmartPlug(SMART_LIGHT_IPS)
        plug.turn_on()
        push_ai_caption("Lights turned on.")
    elif action == "turn off lights":
        plug = SmartPlug(SMART_LIGHT_IPS)
        plug.turn_off()
        push_ai_caption("Lights turned off.")
    nest = Nest(GOOGLE_HOME_EMAIL, GOOGLE_HOME_PASSWORD)
    if action == "set nest to 72":
        nest.set_temperature(72)
        push_ai_caption("Nest set to 72°F.")
    # Placeholder for ADT Control (to be implemented)
    if action == "check adt cameras":
        push_ai_caption("ADT Control camera check not yet implemented.")

def monitor_network():
    try:
        arp = scapy.arping("192.168.1.0/24")  # Adjust to your network
        known_devices = {"192.168.1.1": "Router"}  # Define known devices
        for sent, received in arp[0]:
            if received.psrc not in known_devices and received.psrc != "192.168.1.1":
                pushover_client = pushover.Client(PUSHOVER_USER, api_token=PUSHOVER_TOKEN)
                pushover_client.send_message("New device detected: " + received.psrc, title="Security Alert")
                push_ai_caption(f"New device detected: {received.psrc}")
    except Exception as e:
        print(f"Network monitoring error: {e}")

def health_monitor(symptoms):
    advice = {
        "fever": "Seek medical help if above 100.4°F for 2 days.",
        "cough": "Rest and hydrate; see a doctor if persistent.",
    }
    for symptom in symptoms.split():
        if symptom in advice:
            push_ai_caption(advice[symptom])

def self_modify(new_code):
    temp_file = "/tmp/test_code.py"
    with open(temp_file, 'w') as f:
        f.write(new_code)
    try:
        result = subprocess.run(['python3', temp_file], capture_output=True, text=True)
        if result.returncode == 0 and input("Approve code change? (yes/no): ") == "yes":
            with open(__file__, 'w') as f:
                f.write(new_code)
            push_ai_caption("Code updated successfully.")
        else:
            push_ai_caption("Code test failed or not approved.")
    except Exception as e:
        push_ai_caption(f"Code modification error: {e}")

def main():
    signal.signal(signal.SIGINT, _handle_signal)
    signal.signal(signal.SIGTERM, _handle_signal)
    init_vu(dict(
        SCREEN_SIZE=SCREEN_SIZE, BACKGROUND_PATH=BACKGROUND_PATH, FPS=FPS,
        FONT_PATH=FONT_PATH, TITLE_SIZE=TITLE_SIZE, BODY_SIZE=BODY_SIZE,
        COLOR_WHITE=COLOR_WHITE, COLOR_SHADOW=COLOR_SHADOW, COLOR_GREEN=COLOR_GREEN,
        WAVE_PIXELS=WAVE_PIXELS, WAVE_VISUAL_SCALE=WAVE_VISUAL_SCALE,
        MAX_WAVE_DRAW_PX=MAX_WAVE_DRAW_PX, SCROLL_SPEED_BASE=SCROLL_SPEED_BASE,
        CYCLES1_RANGE=CYCLES1_RANGE, CYCLES2_RANGE=CYCLES2_RANGE
    ))
    threads = [
        threading.Thread(target=display_interface, args=(state, ai_caption_q, talking_event, shutdown_event, push_ai_caption), daemon=True),
        threading.Thread(target=tts_vu_worker, args=(talking_event, shutdown_event), daemon=True),
        threading.Thread(target=evolve_background, args=(state, shutdown_event), daemon=True),
        threading.Thread(target=audio_input_worker, args=(handle_intent_or_ack, talking_event, shutdown_event, MIC_PREFERRED_NAME, LANGUAGE, VOSK_MODEL_PATH, DEBUG_AUDIO), daemon=True),
        threading.Thread(target=start_server, args=(push_ai_caption, HTTP_PORT, shutdown_event), daemon=True),
        threading.Thread(target=monitor_network, args=(), daemon=True)
    ]
    for t in threads:
        t.start()
    try:
        while not shutdown_event.is_set():
            self_diagnose()
            time.sleep(60)  # Check diagnostics every minute
    except KeyboardInterrupt:
        shutdown_event.set()
    finally:
        try:
            if engine:
                engine.stop()
        except Exception:
            pass
        print("Goodbye.")

if __name__ == "__main__":
    main()
```
